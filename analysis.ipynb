{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d468deb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Analisi Avanzata Risultati Performance - perf stat\n",
    "# \n",
    "# Questo notebook analizza i risultati dei test di performance raccolti con `perf stat` per confrontare:\n",
    "# - Implementazioni scalari vs vettoriali\n",
    "# - Tre tipi di operazioni: **standard**, **elementwise**, **sparse**\n",
    "# - Diverse dimensioni di vettori (4, 8, 16, 32)\n",
    "# - Metriche di efficienza avanzate e correlazioni\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurazione per grafici pi√π leggibili\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "\n",
    "# Colori consistenti per le operazioni\n",
    "OPERATION_COLORS = {\n",
    "    'standard': '#1f77b4',\n",
    "    'elementwise': '#ff7f0e', \n",
    "    'sparse': '#2ca02c'\n",
    "}\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Funzioni di Utilit√† Migliorate\n",
    "\n",
    "# %%\n",
    "def parse_perf_csv(file_path):\n",
    "    \"\"\"\n",
    "    Parsa un file CSV di perf stat e restituisce un DataFrame pulito\n",
    "    con gestione migliorata degli errori e metadati pi√π dettagliati\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Leggi il CSV con gestione robusta degli errori\n",
    "        df = pd.read_csv(file_path, header=None, names=[\n",
    "            'value', 'unit', 'event', 'percentage', 'time', 'enabled_time', \n",
    "            'rate', 'description'\n",
    "        ], on_bad_lines='skip')\n",
    "        \n",
    "        # Pulisci e converti i valori numerici\n",
    "        df['value'] = df['value'].astype(str).str.replace(',', '').str.replace('<not counted>', '0')\n",
    "        df['value_numeric'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "        \n",
    "        # Aggiungi metadati dal path del file\n",
    "        path_parts = Path(file_path).parts\n",
    "        df['file_name'] = Path(file_path).stem\n",
    "        df['full_path'] = str(file_path)\n",
    "        df['directory'] = path_parts[-2] if len(path_parts) > 1 else 'unknown'\n",
    "        \n",
    "        # Determina il tipo di test con logica migliorata\n",
    "        path_str = str(file_path).lower()\n",
    "        if 'scalar' in path_str:\n",
    "            df['test_type'] = 'scalar'\n",
    "            df['vector_size'] = 'N/A'\n",
    "        else:\n",
    "            df['test_type'] = 'vector'\n",
    "            # Estrai la dimensione del vettore\n",
    "            vector_size = 'unknown'\n",
    "            for size in [4, 8, 16, 32]:\n",
    "                if f'vector_{size}' in path_str:\n",
    "                    vector_size = str(size)\n",
    "                    break\n",
    "            df['vector_size'] = vector_size\n",
    "        \n",
    "        # Determina il tipo di operazione con logica pi√π robusta\n",
    "        if 'elementwise' in path_str:\n",
    "            df['operation_type'] = 'elementwise'\n",
    "        elif 'sparse' in path_str:\n",
    "            df['operation_type'] = 'sparse'\n",
    "        else:\n",
    "            df['operation_type'] = 'standard'\n",
    "        \n",
    "        # Aggiungi identificatore unico per il test\n",
    "        df['test_id'] = f\"{df['test_type'].iloc[0]}_{df['operation_type'].iloc[0]}_{df['vector_size'].iloc[0]}\"\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel parsing di {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_all_perf_data(base_path='.'):\n",
    "    \"\"\"\n",
    "    Carica tutti i file CSV con gestione migliorata dei pattern\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Pattern pi√π specifici per evitare duplicati\n",
    "    patterns = [\n",
    "        f\"{base_path}/results_scalar/**/*.csv\",\n",
    "        f\"{base_path}/results_vector/**/*.csv\"\n",
    "    ]\n",
    "    \n",
    "    file_count = 0\n",
    "    for pattern in patterns:\n",
    "        csv_files = glob.glob(pattern, recursive=True)\n",
    "        print(f\"Pattern '{pattern}': trovati {len(csv_files)} file CSV\")\n",
    "        \n",
    "        for file_path in csv_files:\n",
    "            df = parse_perf_csv(file_path)\n",
    "            if df is not None and not df.empty:\n",
    "                all_data.append(df)\n",
    "                file_count += 1\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        # Rimuovi duplicati basati su combinazioni di metadati\n",
    "        combined_df = combined_df.drop_duplicates(subset=['file_name', 'event', 'test_type', 'operation_type', 'vector_size'])\n",
    "        print(f\"Caricati {len(combined_df)} record unici da {file_count} file\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"Nessun dato caricato!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_statistical_significance(df, metric, group1, group2, group_col='test_type'):\n",
    "    \"\"\"\n",
    "    Calcola la significativit√† statistica tra due gruppi\n",
    "    \"\"\"\n",
    "    data1 = df[(df[group_col] == group1) & (df['event'] == metric)]['value_numeric'].dropna()\n",
    "    data2 = df[(df[group_col] == group2) & (df['event'] == metric)]['value_numeric'].dropna()\n",
    "    \n",
    "    if len(data1) > 1 and len(data2) > 1:\n",
    "        statistic, p_value = stats.ttest_ind(data1, data2)\n",
    "        return {\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'mean_diff': data1.mean() - data2.mean(),\n",
    "            'effect_size': (data1.mean() - data2.mean()) / np.sqrt((data1.var() + data2.var()) / 2)\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Caricamento e Validazione Dati\n",
    "\n",
    "# %%\n",
    "# Carica tutti i dati\n",
    "df = load_all_perf_data()\n",
    "\n",
    "# Validazione e pulizia dati\n",
    "if not df.empty:\n",
    "    print(\"=\"*60)\n",
    "    print(\"DATASET CARICATO CON SUCCESSO\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Forma del dataset: {df.shape}\")\n",
    "    print(f\"Tipi di test: {sorted(df['test_type'].unique())}\")\n",
    "    print(f\"Dimensioni vettori: {sorted(df[df['vector_size'] != 'N/A']['vector_size'].unique(), key=lambda x: int(x) if x.isdigit() else 0)}\")\n",
    "    print(f\"Tipi di operazioni: {sorted(df['operation_type'].unique())}\")\n",
    "    print(f\"Eventi misurati: {sorted(df['event'].unique())}\")\n",
    "    \n",
    "    # Conta test per categoria\n",
    "    print(\"\\nDistribuzione test per categoria:\")\n",
    "    test_distribution = df.groupby(['test_type', 'operation_type', 'vector_size']).size().reset_index(name='count')\n",
    "    for _, row in test_distribution.iterrows():\n",
    "        print(f\"  {row['test_type']:8s} | {row['operation_type']:12s} | Size {row['vector_size']:3s} | {row['count']:3d} metriche\")\n",
    "    \n",
    "    # Verifica qualit√† dati\n",
    "    print(f\"\\nQualit√† dati:\")\n",
    "    print(f\"  Valori mancanti: {df['value_numeric'].isna().sum()}/{len(df)} ({df['value_numeric'].isna().mean()*100:.1f}%)\")\n",
    "    print(f\"  Valori zero: {(df['value_numeric'] == 0).sum()}\")\n",
    "    print(f\"  File unici: {df['file_name'].nunique()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERRORE: Dataset vuoto. Verifica i percorsi dei file.\")\n",
    "    exit()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Preparazione Metriche Principali\n",
    "\n",
    "# %%\n",
    "# Metriche chiave con descrizioni\n",
    "KEY_METRICS = {\n",
    "    'task-clock': 'Tempo di esecuzione (ms)',\n",
    "    'cycles': 'Cicli CPU totali',\n",
    "    'instructions': 'Istruzioni eseguite',\n",
    "    'branches': 'Branch totali',\n",
    "    'branch-misses': 'Branch mancati',\n",
    "    'cache-references': 'Accessi cache',\n",
    "    'cache-misses': 'Cache miss',\n",
    "}\n",
    "\n",
    "# Filtra solo le metriche principali disponibili\n",
    "available_metrics = [metric for metric in KEY_METRICS.keys() if metric in df['event'].values]\n",
    "print(f\"Metriche disponibili per l'analisi: {available_metrics}\")\n",
    "\n",
    "df_metrics = df[df['event'].isin(available_metrics)].copy()\n",
    "print(f\"Dataset filtrato: {len(df_metrics)} record con metriche principali\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. ANALISI COMPARATIVA TRA TIPI DI OPERAZIONI\n",
    "\n",
    "# %%\n",
    "def create_operation_comparison_plots():\n",
    "    \"\"\"\n",
    "    Crea grafici di confronto dettagliati tra i tre tipi di operazioni\n",
    "    \"\"\"\n",
    "    if df_metrics.empty:\n",
    "        print(\"Nessun dato disponibile per il confronto\")\n",
    "        return\n",
    "    \n",
    "    # Configura la griglia di subplot\n",
    "    n_metrics = len(available_metrics)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    fig.suptitle('Confronto Dettagliato tra Tipi di Operazioni\\n(Standard vs Elementwise vs Sparse)', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    for i, metric in enumerate(available_metrics):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        metric_data = df_metrics[df_metrics['event'] == metric].copy()\n",
    "        \n",
    "        if not metric_data.empty:\n",
    "            # Box plot con overlay dei punti\n",
    "            sns.boxplot(data=metric_data, x='operation_type', y='value_numeric', \n",
    "                       palette=OPERATION_COLORS, ax=ax)\n",
    "            sns.stripplot(data=metric_data, x='operation_type', y='value_numeric', \n",
    "                         color='black', alpha=0.4, size=3, ax=ax)\n",
    "            \n",
    "            ax.set_title(f'{KEY_METRICS.get(metric, metric)}', fontweight='bold')\n",
    "            ax.set_ylabel('Valore')\n",
    "            ax.set_xlabel('Tipo Operazione')\n",
    "            \n",
    "            # Aggiungi statistiche descrittive\n",
    "            stats_text = []\n",
    "            for op_type in metric_data['operation_type'].unique():\n",
    "                values = metric_data[metric_data['operation_type'] == op_type]['value_numeric']\n",
    "                if not values.empty:\n",
    "                    mean_val = values.mean()\n",
    "                    std_val = values.std()\n",
    "                    stats_text.append(f'{op_type}: Œº={mean_val:.2e}¬±{std_val:.1e}')\n",
    "            \n",
    "            ax.text(0.02, 0.98, '\\n'.join(stats_text), transform=ax.transAxes, \n",
    "                   verticalalignment='top', fontsize=8, bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                   facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    # Rimuovi subplot vuoti\n",
    "    for i in range(len(available_metrics), n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        if row < n_rows and col < n_cols:\n",
    "            axes[row, col].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Esegui l'analisi comparativa\n",
    "create_operation_comparison_plots()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. ANALISI STATISTICA APPROFONDITA\n",
    "\n",
    "# %%\n",
    "def perform_statistical_analysis():\n",
    "    \"\"\"\n",
    "    Esegue analisi statistica approfondita tra i tipi di operazioni\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ANALISI STATISTICA APPROFONDITA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test statistici per ogni metrica\n",
    "    for metric in available_metrics:\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"METRICA: {KEY_METRICS.get(metric, metric).upper()}\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        metric_data = df_metrics[df_metrics['event'] == metric]\n",
    "        \n",
    "        # Statistiche descrittive per tipo di operazione\n",
    "        desc_stats = metric_data.groupby('operation_type')['value_numeric'].agg([\n",
    "            'count', 'mean', 'std', 'min', 'max', 'median'\n",
    "        ]).round(2)\n",
    "        \n",
    "        print(\"\\nStatistiche Descrittive:\")\n",
    "        print(desc_stats.to_string())\n",
    "        \n",
    "        # Test di significativit√† tra tutte le coppie\n",
    "        operations = metric_data['operation_type'].unique()\n",
    "        if len(operations) >= 2:\n",
    "            print(f\"\\nTest t-Student per coppie di operazioni:\")\n",
    "            for op1, op2 in combinations(operations, 2):\n",
    "                sig_test = calculate_statistical_significance(\n",
    "                    metric_data, metric, op1, op2, 'operation_type'\n",
    "                )\n",
    "                if sig_test:\n",
    "                    significance = \"***\" if sig_test['p_value'] < 0.001 else \"**\" if sig_test['p_value'] < 0.01 else \"*\" if sig_test['p_value'] < 0.05 else \"ns\"\n",
    "                    print(f\"  {op1:12s} vs {op2:12s}: p={sig_test['p_value']:.4f} {significance:3s} \"\n",
    "                          f\"(Effect size: {sig_test['effect_size']:.2f})\")\n",
    "        \n",
    "        # Calcola miglioramenti percentuali rispetto al baseline (standard)\n",
    "        if 'standard' in operations:\n",
    "            standard_mean = metric_data[metric_data['operation_type'] == 'standard']['value_numeric'].mean()\n",
    "            print(f\"\\nMiglioramenti rispetto a 'standard':\")\n",
    "            for op in operations:\n",
    "                if op != 'standard':\n",
    "                    op_mean = metric_data[metric_data['operation_type'] == op]['value_numeric'].mean()\n",
    "                    if standard_mean > 0:\n",
    "                        improvement = ((standard_mean - op_mean) / standard_mean) * 100\n",
    "                        direction = \"miglioramento\" if improvement > 0 else \"peggioramento\"\n",
    "                        print(f\"  {op:12s}: {improvement:+6.1f}% ({direction})\")\n",
    "\n",
    "# Esegui l'analisi statistica\n",
    "perform_statistical_analysis()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. ANALISI PER DIMENSIONE VETTORE\n",
    "\n",
    "# %%\n",
    "def analyze_vector_dimensions():\n",
    "    \"\"\"\n",
    "    Analizza le performance per diverse dimensioni di vettori\n",
    "    \"\"\"\n",
    "    df_vector = df_metrics[df_metrics['test_type'] == 'vector'].copy()\n",
    "    \n",
    "    if df_vector.empty:\n",
    "        print(\"Nessun dato vettoriale disponibile\")\n",
    "        return\n",
    "    \n",
    "    # Ordina le dimensioni\n",
    "    vector_sizes = sorted([size for size in df_vector['vector_size'].unique() \n",
    "                          if size not in ['N/A', 'unknown']], \n",
    "                         key=lambda x: int(x) if x.isdigit() else 0)\n",
    "    \n",
    "    if not vector_sizes:\n",
    "        print(\"Nessuna dimensione vettore valida trovata\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dimensioni vettore analizzate: {vector_sizes}\")\n",
    "    \n",
    "    # Crea heatmap per ogni tipo di operazione\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    fig.suptitle('Performance per Dimensione Vettore e Tipo Operazione', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, op_type in enumerate(['standard', 'elementwise', 'sparse']):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        op_data = df_vector[df_vector['operation_type'] == op_type]\n",
    "        \n",
    "        if not op_data.empty:\n",
    "            # Crea matrice pivot per heatmap\n",
    "            pivot_data = op_data.pivot_table(\n",
    "                index='event', \n",
    "                columns='vector_size', \n",
    "                values='value_numeric',\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            # Normalizza per riga per confronto relativo\n",
    "            pivot_normalized = pivot_data.div(pivot_data.mean(axis=1), axis=0)\n",
    "            \n",
    "            sns.heatmap(pivot_normalized, annot=True, fmt='.2f', cmap='RdYlBu_r', \n",
    "                       center=1, ax=axes[i], cbar_kws={'label': 'Valore Relativo'})\n",
    "            axes[i].set_title(f'Operazioni {op_type.title()}')\n",
    "            axes[i].set_xlabel('Dimensione Vettore')\n",
    "            axes[i].set_ylabel('Metrica')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analisi della scalabilit√†\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALISI SCALABILIT√Ä DIMENSIONI VETTORE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for metric in ['task-clock', 'cycles', 'instructions']:\n",
    "        if metric not in available_metrics:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{metric.upper()}:\")\n",
    "        \n",
    "        for op_type in ['standard', 'elementwise', 'sparse']:\n",
    "            op_data = df_vector[(df_vector['operation_type'] == op_type) & \n",
    "                               (df_vector['event'] == metric)]\n",
    "            \n",
    "            if not op_data.empty:\n",
    "                print(f\"  {op_type:12s}:\", end=\"\")\n",
    "                for size in vector_sizes:\n",
    "                    size_data = op_data[op_data['vector_size'] == size]['value_numeric']\n",
    "                    if not size_data.empty:\n",
    "                        mean_val = size_data.mean()\n",
    "                        print(f\" Size {size}: {mean_val:.2e}\", end=\" |\")\n",
    "                print()\n",
    "\n",
    "# Esegui analisi dimensioni vettore\n",
    "analyze_vector_dimensions()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. METRICHE DI EFFICIENZA AVANZATE\n",
    "\n",
    "# %%\n",
    "def calculate_advanced_efficiency_metrics():\n",
    "    \"\"\"\n",
    "    Calcola metriche di efficienza avanzate per ogni combinazione di test\n",
    "    \"\"\"\n",
    "    efficiency_data = []\n",
    "    \n",
    "    # Raggruppa per test unico\n",
    "    for test_group in df_metrics.groupby(['file_name', 'test_type', 'operation_type', 'vector_size']):\n",
    "        group_key, group_data = test_group\n",
    "        \n",
    "        # Estrai valori metriche\n",
    "        metrics_dict = {}\n",
    "        for _, row in group_data.iterrows():\n",
    "            metrics_dict[row['event']] = row['value_numeric']\n",
    "        \n",
    "        # Calcola metriche derivate\n",
    "        efficiency_record = {\n",
    "            'file_name': group_key[0],\n",
    "            'test_type': group_key[1],\n",
    "            'operation_type': group_key[2],\n",
    "            'vector_size': group_key[3],\n",
    "        }\n",
    "        \n",
    "        # IPC (Instructions Per Cycle)\n",
    "        if 'cycles' in metrics_dict and 'instructions' in metrics_dict:\n",
    "            cycles = metrics_dict['cycles']\n",
    "            instructions = metrics_dict['instructions']\n",
    "            efficiency_record['ipc'] = instructions / cycles if cycles > 0 else np.nan\n",
    "            efficiency_record['cycles_per_instruction'] = cycles / instructions if instructions > 0 else np.nan\n",
    "        \n",
    "        # Branch prediction efficiency\n",
    "        if 'branches' in metrics_dict and 'branch-misses' in metrics_dict:\n",
    "            branches = metrics_dict['branches']\n",
    "            branch_misses = metrics_dict['branch-misses']\n",
    "            efficiency_record['branch_miss_rate'] = (branch_misses / branches * 100) if branches > 0 else np.nan\n",
    "            efficiency_record['branch_accuracy'] = ((branches - branch_misses) / branches * 100) if branches > 0 else np.nan\n",
    "        \n",
    "        # Throughput metrics\n",
    "        if 'task-clock' in metrics_dict:\n",
    "            time_ms = metrics_dict['task-clock']\n",
    "            if 'instructions' in metrics_dict:\n",
    "                efficiency_record['instructions_per_ms'] = metrics_dict['instructions'] / time_ms if time_ms > 0 else np.nan\n",
    "            if 'cycles' in metrics_dict:\n",
    "                efficiency_record['cycles_per_ms'] = metrics_dict['cycles'] / time_ms if time_ms > 0 else np.nan\n",
    "        \n",
    "        # Cache efficiency (se disponibile)\n",
    "        if 'cache-references' in metrics_dict and 'cache-misses' in metrics_dict:\n",
    "            cache_refs = metrics_dict['cache-references']\n",
    "            cache_misses = metrics_dict['cache-misses']\n",
    "            efficiency_record['cache_miss_rate'] = (cache_misses / cache_refs * 100) if cache_refs > 0 else np.nan\n",
    "            efficiency_record['cache_hit_rate'] = ((cache_refs - cache_misses) / cache_refs * 100) if cache_refs > 0 else np.nan\n",
    "        \n",
    "        efficiency_data.append(efficiency_record)\n",
    "    \n",
    "    df_efficiency = pd.DataFrame(efficiency_data)\n",
    "    \n",
    "    # Visualizza metriche di efficienza\n",
    "    efficiency_metrics = ['ipc', 'branch_miss_rate', 'instructions_per_ms', 'cache_miss_rate']\n",
    "    available_eff_metrics = [m for m in efficiency_metrics if m in df_efficiency.columns and not df_efficiency[m].isna().all()]\n",
    "    \n",
    "    if available_eff_metrics:\n",
    "        n_metrics = len(available_eff_metrics)\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        fig.suptitle('Metriche di Efficienza Avanzate per Tipo di Operazione', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, metric in enumerate(available_eff_metrics[:4]):\n",
    "            if i >= len(axes):\n",
    "                break\n",
    "                \n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Box plot per tipo di operazione\n",
    "            metric_data = df_efficiency.dropna(subset=[metric])\n",
    "            if not metric_data.empty:\n",
    "                sns.boxplot(data=metric_data, x='operation_type', y=metric, \n",
    "                           palette=OPERATION_COLORS, ax=ax)\n",
    "                \n",
    "                # Aggiungi overlay dei punti per test type\n",
    "                sns.stripplot(data=metric_data, x='operation_type', y=metric, \n",
    "                             hue='test_type', alpha=0.7, size=4, ax=ax)\n",
    "                \n",
    "                ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "                ax.set_xlabel('Tipo Operazione')\n",
    "                \n",
    "                # Calcola e mostra statistiche\n",
    "                for op_type in metric_data['operation_type'].unique():\n",
    "                    values = metric_data[metric_data['operation_type'] == op_type][metric]\n",
    "                    if not values.empty:\n",
    "                        mean_val = values.mean()\n",
    "                        ax.axhline(y=mean_val, color=OPERATION_COLORS.get(op_type, 'gray'), \n",
    "                                  linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Rimuovi subplot vuoti\n",
    "        for i in range(len(available_eff_metrics), 4):\n",
    "            if i < len(axes):\n",
    "                axes[i].remove()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return df_efficiency\n",
    "\n",
    "# Calcola metriche di efficienza\n",
    "df_efficiency = calculate_advanced_efficiency_metrics()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. ANALISI DI CORRELAZIONE AVANZATA\n",
    "\n",
    "# %%\n",
    "def create_correlation_analysis():\n",
    "    \"\"\"\n",
    "    Crea analisi di correlazione avanzata tra metriche\n",
    "    \"\"\"\n",
    "    # Prepara dati per correlazione\n",
    "    pivot_data = df_metrics.pivot_table(\n",
    "        index=['file_name', 'test_type', 'operation_type', 'vector_size'], \n",
    "        columns='event', \n",
    "        values='value_numeric',\n",
    "        aggfunc='mean'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Correlazione generale\n",
    "    numeric_cols = [col for col in pivot_data.columns if col in available_metrics]\n",
    "    if len(numeric_cols) >= 2:\n",
    "        correlation_matrix = pivot_data[numeric_cols].corr()\n",
    "        \n",
    "        # Heatmap correlazione\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0, \n",
    "                    square=True, fmt='.3f', cbar_kws={'label': 'Correlazione'})\n",
    "        plt.title('Matrice di Correlazione tra Metriche di Performance', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Correlazione per tipo di operazione\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('Correlazioni per Tipo di Operazione', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, op_type in enumerate(['standard', 'elementwise', 'sparse']):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        op_data = pivot_data[pivot_data['operation_type'] == op_type]\n",
    "        \n",
    "        if len(op_data) > 1 and len(numeric_cols) >= 2:\n",
    "            op_corr = op_data[numeric_cols].corr()\n",
    "            \n",
    "            sns.heatmap(op_corr, annot=True, cmap='coolwarm', center=0, \n",
    "                       square=True, fmt='.2f', ax=axes[i],\n",
    "                       cbar_kws={'label': 'Correlazione'})\n",
    "            axes[i].set_title(f'Operazioni {op_type.title()}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Esegui analisi correlazione\n",
    "create_correlation_analysis()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. PERFORMANCE RANKING E RACCOMANDAZIONI\n",
    "\n",
    "# %%\n",
    "def generate_performance_ranking():\n",
    "    \"\"\"\n",
    "    Genera ranking delle performance e raccomandazioni\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"RANKING PERFORMANCE E RACCOMANDAZIONI\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Ranking per tempo di esecuzione\n",
    "    if 'task-clock' in available_metrics:\n",
    "        time_data = df_metrics[df_metrics['event'] == 'task-clock'].groupby(\n",
    "            ['operation_type', 'test_type', 'vector_size']\n",
    "        )['value_numeric'].mean().reset_index()\n",
    "        \n",
    "        print(\"\\nüèÜ RANKING TEMPO DI ESECUZIONE (migliore = pi√π veloce):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        time_ranking = time_data.sort_values('value_numeric')\n",
    "        for i, (_, row) in enumerate(time_ranking.head(10).iterrows(), 1):\n",
    "            config = f\"{row['test_type']}-{row['operation_type']}\"\n",
    "            if row['vector_size'] != 'N/A':\n",
    "                config += f\"-size{row['vector_size']}\"\n",
    "            print(f\"{i:2d}. {config:25s}: {row['value_numeric']:8.3f} ms\")\n",
    "    \n",
    "    # Ranking per efficienza istruzioni\n",
    "    if not df_efficiency.empty and 'ipc' in df_efficiency.columns:\n",
    "        ipc_data = df_efficiency.dropna(subset=['ipc']).groupby(\n",
    "            ['operation_type', 'test_type', 'vector_size']\n",
    "        )['ipc'].mean().reset_index()\n",
    "        \n",
    "        print(f\"\\nüìä RANKING EFFICIENZA IPC (migliore = pi√π alto):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        ipc_ranking = ipc_data.sort_values('ipc', ascending=False)\n",
    "        for i, (_, row) in enumerate(ipc_ranking.head(10).iterrows(), 1):\n",
    "            config = f\"{row['test_type']}-{row['operation_type']}\"\n",
    "            if row['vector_size'] != 'N/A':\n",
    "                config += f\"-size{row['vector_size']}\"\n",
    "            print(f\"{i:2d}. {config:25s}: {row['ipc']:8.3f} IPC\")\n",
    "    \n",
    "    # Raccomandazioni basate sui dati\n",
    "    print(f\"\\nüí° RACCOMANDAZIONI:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Raccomandazione per tipo di operazione\n",
    "    if 'task-clock' in available_metrics:\n",
    "        op_performance = df_metrics[df_metrics['event'] == 'task-clock'].groupby('operation_type')['value_numeric'].mean()\n",
    "        best_operation = op_performance.idxmin()\n",
    "        worst_operation = op_performance.idxmax()\n",
    "        \n",
    "        improvement = ((op_performance[worst_operation] - op_performance[best_operation]) / op_performance[worst_operation]) * 100\n",
    "        \n",
    "        print(f\"1. üéØ Tipo operazione ottimale: '{best_operation.upper()}'\")\n",
    "        print(f\"   - {improvement:.1f}% pi√π veloce rispetto a '{worst_operation}'\")\n",
    "    \n",
    "    # Raccomandazione per dimensione vettore\n",
    "    if 'task-clock' in available_metrics:\n",
    "        vector_data = df_metrics[(df_metrics['event'] == 'task-clock') & (df_metrics['test_type'] == 'vector')]\n",
    "        if not vector_data.empty:\n",
    "            size_performance = vector_data.groupby('vector_size')['value_numeric'].mean()\n",
    "        valid_sizes = [size for size in size_performance.index if size not in ['N/A', 'unknown']]\n",
    "        \n",
    "        if valid_sizes:\n",
    "            # Ordina per performance (tempo minore = migliore)\n",
    "            size_performance_valid = size_performance[valid_sizes].sort_values()\n",
    "            best_size = size_performance_valid.index[0]\n",
    "            worst_size = size_performance_valid.index[-1]\n",
    "            \n",
    "            size_improvement = ((size_performance_valid[worst_size] - size_performance_valid[best_size]) / size_performance_valid[worst_size]) * 100\n",
    "            \n",
    "            print(f\"2. üìê Dimensione vettore ottimale: Size {best_size}\")\n",
    "            print(f\"   - {size_improvement:.1f}% pi√π veloce rispetto a Size {worst_size}\")\n",
    "    \n",
    "    # Raccomandazione per cache efficiency\n",
    "    if not df_efficiency.empty and 'cache_miss_rate' in df_efficiency.columns:\n",
    "        cache_data = df_efficiency.dropna(subset=['cache_miss_rate'])\n",
    "        if not cache_data.empty:\n",
    "            best_cache_op = cache_data.loc[cache_data['cache_miss_rate'].idxmin(), 'operation_type']\n",
    "            best_cache_rate = cache_data['cache_miss_rate'].min()\n",
    "            \n",
    "            print(f\"3. üéØ Migliore efficienza cache: '{best_cache_op.upper()}'\")\n",
    "            print(f\"   - Cache miss rate: {best_cache_rate:.2f}%\")\n",
    "    \n",
    "    # Raccomandazione per branch prediction\n",
    "    if not df_efficiency.empty and 'branch_miss_rate' in df_efficiency.columns:\n",
    "        branch_data = df_efficiency.dropna(subset=['branch_miss_rate'])\n",
    "        if not branch_data.empty:\n",
    "            best_branch_op = branch_data.loc[branch_data['branch_miss_rate'].idxmin(), 'operation_type']\n",
    "            best_branch_rate = branch_data['branch_miss_rate'].min()\n",
    "            \n",
    "            print(f\"4. üîÑ Migliore branch prediction: '{best_branch_op.upper()}'\")\n",
    "            print(f\"   - Branch miss rate: {best_branch_rate:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüìà CONSIDERAZIONI GENERALI:\")\n",
    "    print(f\"   - Confronta sempre scalar vs vector per la tua use case specifica\")\n",
    "    print(f\"   - Considera il trade-off tra complessit√† e performance\")\n",
    "    print(f\"   - Test su dataset reali potrebbero dare risultati diversi\")\n",
    "    print(f\"   - Le dimensioni ottimali dipendono dal tipo di operazione\")\n",
    "\n",
    "# Esegui ranking e raccomandazioni\n",
    "generate_performance_ranking()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. GRAFICI INTERATTIVI CON PLOTLY\n",
    "\n",
    "# %%\n",
    "def create_interactive_dashboard():\n",
    "    \"\"\"\n",
    "    Crea dashboard interattivo con Plotly per esplorazione avanzata\n",
    "    \"\"\"\n",
    "    if df_metrics.empty:\n",
    "        print(\"Nessun dato disponibile per dashboard interattivo\")\n",
    "        return\n",
    "    \n",
    "    # Dashboard multi-metrica\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Tempo Esecuzione per Operazione', 'Cicli CPU per Dimensione', \n",
    "                       'Distribuzione IPC', 'Correlazione Metriche'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Grafico 1: Tempo di esecuzione\n",
    "    if 'task-clock' in available_metrics:\n",
    "        time_data = df_metrics[df_metrics['event'] == 'task-clock']\n",
    "        \n",
    "        for op_type in time_data['operation_type'].unique():\n",
    "            op_data = time_data[time_data['operation_type'] == op_type]\n",
    "            fig.add_trace(\n",
    "                go.Box(y=op_data['value_numeric'], name=f'{op_type} Time',\n",
    "                      marker_color=OPERATION_COLORS.get(op_type, 'gray'),\n",
    "                      showlegend=True),\n",
    "                row=1, col=1\n",
    "            )\n",
    "    \n",
    "    # Grafico 2: Cicli per dimensione vettore\n",
    "    if 'cycles' in available_metrics:\n",
    "        cycles_data = df_metrics[(df_metrics['event'] == 'cycles') & (df_metrics['test_type'] == 'vector')]\n",
    "        \n",
    "        for op_type in cycles_data['operation_type'].unique():\n",
    "            op_data = cycles_data[cycles_data['operation_type'] == op_type]\n",
    "            if not op_data.empty:\n",
    "                # Raggruppa per dimensione\n",
    "                size_means = op_data.groupby('vector_size')['value_numeric'].mean().reset_index()\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=size_means['vector_size'], y=size_means['value_numeric'],\n",
    "                             mode='lines+markers', name=f'{op_type} Cycles',\n",
    "                             line=dict(color=OPERATION_COLORS.get(op_type, 'gray'))),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "    \n",
    "    # Grafico 3: Distribuzione IPC\n",
    "    if not df_efficiency.empty and 'ipc' in df_efficiency.columns:\n",
    "        ipc_data = df_efficiency.dropna(subset=['ipc'])\n",
    "        \n",
    "        for op_type in ipc_data['operation_type'].unique():\n",
    "            op_ipc = ipc_data[ipc_data['operation_type'] == op_type]['ipc']\n",
    "            fig.add_trace(\n",
    "                go.Histogram(x=op_ipc, name=f'{op_type} IPC', \n",
    "                           opacity=0.7, nbinsx=15,\n",
    "                           marker_color=OPERATION_COLORS.get(op_type, 'gray')),\n",
    "                row=2, col=1\n",
    "            )\n",
    "    \n",
    "    # Grafico 4: Heatmap correlazione (semplificata)\n",
    "    if len(available_metrics) >= 2:\n",
    "        pivot_data = df_metrics.pivot_table(\n",
    "            index=['file_name'], columns='event', values='value_numeric', aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        if not pivot_data.empty:\n",
    "            corr_matrix = pivot_data[available_metrics[:4]].corr()  # Prime 4 metriche\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Heatmap(z=corr_matrix.values, \n",
    "                          x=corr_matrix.columns, \n",
    "                          y=corr_matrix.index,\n",
    "                          colorscale='RdBu', zmid=0,\n",
    "                          text=corr_matrix.round(2).values,\n",
    "                          texttemplate=\"%{text}\", textfont={\"size\":10}),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    # Aggiorna layout\n",
    "    fig.update_layout(\n",
    "        height=800, \n",
    "        title_text=\"Dashboard Interattivo Performance Analysis\",\n",
    "        title_x=0.5,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Crea dashboard interattivo\n",
    "create_interactive_dashboard()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. EXPORT RISULTATI E SUMMARY FINALE\n",
    "\n",
    "# %%\n",
    "def export_analysis_results():\n",
    "    \"\"\"\n",
    "    Esporta risultati dell'analisi in formati utilizzabili\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"EXPORT RISULTATI ANALISI\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Crea cartella per export se non esistente\n",
    "    output_dir = Path(\"analysis_results\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Export dataset principale\n",
    "    if not df_metrics.empty:\n",
    "        df_metrics.to_csv(output_dir / \"performance_metrics_cleaned.csv\", index=False)\n",
    "        print(f\"‚úÖ Dataset metriche esportato: {len(df_metrics)} record\")\n",
    "    \n",
    "    # 2. Export metriche di efficienza\n",
    "    if not df_efficiency.empty:\n",
    "        df_efficiency.to_csv(output_dir / \"efficiency_metrics.csv\", index=False)\n",
    "        print(f\"‚úÖ Metriche efficienza esportate: {len(df_efficiency)} record\")\n",
    "    \n",
    "    # 3. Export summary statistiche\n",
    "    summary_stats = []\n",
    "    \n",
    "    for metric in available_metrics:\n",
    "        metric_data = df_metrics[df_metrics['event'] == metric]\n",
    "        \n",
    "        for op_type in metric_data['operation_type'].unique():\n",
    "            op_data = metric_data[metric_data['operation_type'] == op_type]['value_numeric']\n",
    "            \n",
    "            if not op_data.empty:\n",
    "                summary_stats.append({\n",
    "                    'metric': metric,\n",
    "                    'operation_type': op_type,\n",
    "                    'count': len(op_data),\n",
    "                    'mean': op_data.mean(),\n",
    "                    'std': op_data.std(),\n",
    "                    'min': op_data.min(),\n",
    "                    'max': op_data.max(),\n",
    "                    'median': op_data.median(),\n",
    "                    'q25': op_data.quantile(0.25),\n",
    "                    'q75': op_data.quantile(0.75)\n",
    "                })\n",
    "    \n",
    "    if summary_stats:\n",
    "        pd.DataFrame(summary_stats).to_csv(output_dir / \"summary_statistics.csv\", index=False)\n",
    "        print(f\"‚úÖ Statistiche riassuntive esportate\")\n",
    "    \n",
    "    # 4. Export best configurations\n",
    "    best_configs = []\n",
    "    \n",
    "    if 'task-clock' in available_metrics:\n",
    "        time_data = df_metrics[df_metrics['event'] == 'task-clock']\n",
    "        best_time = time_data.loc[time_data['value_numeric'].idxmin()]\n",
    "        best_configs.append({\n",
    "            'metric': 'best_execution_time',\n",
    "            'configuration': f\"{best_time['test_type']}-{best_time['operation_type']}-{best_time['vector_size']}\",\n",
    "            'value': best_time['value_numeric'],\n",
    "            'unit': 'ms'\n",
    "        })\n",
    "    \n",
    "    if not df_efficiency.empty and 'ipc' in df_efficiency.columns:\n",
    "        ipc_data = df_efficiency.dropna(subset=['ipc'])\n",
    "        if not ipc_data.empty:\n",
    "            best_ipc = ipc_data.loc[ipc_data['ipc'].idxmax()]\n",
    "            best_configs.append({\n",
    "                'metric': 'best_ipc',\n",
    "                'configuration': f\"{best_ipc['test_type']}-{best_ipc['operation_type']}-{best_ipc['vector_size']}\",\n",
    "                'value': best_ipc['ipc'],\n",
    "                'unit': 'instructions/cycle'\n",
    "            })\n",
    "    \n",
    "    if best_configs:\n",
    "        pd.DataFrame(best_configs).to_csv(output_dir / \"best_configurations.csv\", index=False)\n",
    "        print(f\"‚úÖ Configurazioni ottimali esportate\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Tutti i file esportati in: {output_dir.absolute()}\")\n",
    "\n",
    "def generate_final_summary():\n",
    "    \"\"\"\n",
    "    Genera summary finale dell'analisi\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY FINALE DELL'ANALISI\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüìä DATASET ANALIZZATO:\")\n",
    "    print(f\"   ‚Ä¢ File processati: {df['file_name'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Record totali: {len(df)}\")\n",
    "    print(f\"   ‚Ä¢ Metriche principali: {len(available_metrics)}\")\n",
    "    print(f\"   ‚Ä¢ Tipi operazione: {', '.join(df['operation_type'].unique())}\")\n",
    "    print(f\"   ‚Ä¢ Dimensioni vettore: {', '.join(sorted([s for s in df['vector_size'].unique() if s not in ['N/A', 'unknown']]))}\")\n",
    "    \n",
    "    # Migliori performance per categoria\n",
    "    print(f\"\\nüèÜ PERFORMANCE HIGHLIGHTS:\")\n",
    "    \n",
    "    if 'task-clock' in available_metrics:\n",
    "        fastest_overall = df_metrics[df_metrics['event'] == 'task-clock'].loc[\n",
    "            df_metrics[df_metrics['event'] == 'task-clock']['value_numeric'].idxmin()\n",
    "        ]\n",
    "        print(f\"   ‚Ä¢ Configurazione pi√π veloce:\")\n",
    "        print(f\"     {fastest_overall['test_type']}-{fastest_overall['operation_type']} (Size: {fastest_overall['vector_size']}) = {fastest_overall['value_numeric']:.3f} ms\")\n",
    "    \n",
    "    if not df_efficiency.empty and 'ipc' in df_efficiency.columns:\n",
    "        best_efficiency = df_efficiency.loc[df_efficiency['ipc'].idxmax()]\n",
    "        print(f\"   ‚Ä¢ Migliore efficienza IPC:\")\n",
    "        print(f\"     {best_efficiency['test_type']}-{best_efficiency['operation_type']} (Size: {best_efficiency['vector_size']}) = {best_efficiency['ipc']:.3f} IPC\")\n",
    "    \n",
    "    # Insights principali\n",
    "    print(f\"\\nüí° INSIGHTS CHIAVE:\")\n",
    "    \n",
    "    # Confronto scalar vs vector\n",
    "    if 'task-clock' in available_metrics:\n",
    "        scalar_time = df_metrics[(df_metrics['event'] == 'task-clock') & (df_metrics['test_type'] == 'scalar')]['value_numeric'].mean()\n",
    "        vector_time = df_metrics[(df_metrics['event'] == 'task-clock') & (df_metrics['test_type'] == 'vector')]['value_numeric'].mean()\n",
    "        \n",
    "        if not pd.isna(scalar_time) and not pd.isna(vector_time):\n",
    "            speedup = scalar_time / vector_time if vector_time > 0 else np.nan\n",
    "            if not pd.isna(speedup):\n",
    "                print(f\"   ‚Ä¢ Speedup medio vector vs scalar: {speedup:.2f}x\")\n",
    "    \n",
    "    # Migliore tipo di operazione\n",
    "    if 'task-clock' in available_metrics:\n",
    "        op_times = df_metrics[df_metrics['event'] == 'task-clock'].groupby('operation_type')['value_numeric'].mean()\n",
    "        if not op_times.empty:\n",
    "            best_op = op_times.idxmin()\n",
    "            worst_op = op_times.idxmax()\n",
    "            improvement = ((op_times[worst_op] - op_times[best_op]) / op_times[worst_op]) * 100\n",
    "            print(f\"   ‚Ä¢ Operazione pi√π efficiente: {best_op} ({improvement:.1f}% migliore di {worst_op})\")\n",
    "    \n",
    "    # Tendenza scalabilit√†\n",
    "    vector_data = df_metrics[(df_metrics['test_type'] == 'vector') & (df_metrics['event'] == 'task-clock')]\n",
    "    if not vector_data.empty:\n",
    "        size_performance = vector_data.groupby('vector_size')['value_numeric'].mean()\n",
    "        valid_sizes = [s for s in size_performance.index if s not in ['N/A', 'unknown'] and s.isdigit()]\n",
    "        \n",
    "        if len(valid_sizes) >= 2:\n",
    "            # Ordina numericamente\n",
    "            valid_sizes_sorted = sorted(valid_sizes, key=int)\n",
    "            trend = \"crescente\" if size_performance[valid_sizes_sorted[-1]] > size_performance[valid_sizes_sorted[0]] else \"decrescente\"\n",
    "            print(f\"   ‚Ä¢ Trend scalabilit√† dimensioni: {trend}\")\n",
    "    \n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Esegui export e summary finale\n",
    "export_analysis_results()\n",
    "generate_final_summary()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Conclusioni\n",
    "# \n",
    "# Questa analisi completa ha fornito:\n",
    "# \n",
    "# 1. **Confronto dettagliato** tra implementazioni scalari e vettoriali\n",
    "# 2. **Analisi statistica** con test di significativit√†\n",
    "# 3. **Metriche di efficienza avanzate** (IPC, cache efficiency, branch prediction)\n",
    "# 4. **Correlazioni** tra diverse metriche di performance\n",
    "# 5. **Ranking** delle configurazioni migliori\n",
    "# 6. **Dashboard interattivo** per esplorazione dinamica\n",
    "# 7. **Export** di tutti i risultati per uso futuro\n",
    "# \n",
    "# I risultati mostrano chiaramente quali configurazioni offrono le migliori performance per ciascun tipo di operazione e dimensione vettore, fornendo una base solida per decisioni di ottimizzazione del codice.\n",
    "\n",
    "# %%\n",
    "print(\"‚úÖ ANALISI COMPLETATA CON SUCCESSO!\")\n",
    "print(\"üìä Tutti i grafici, statistiche e export sono stati generati.\")\n",
    "print(\"üîç Consulta i file nella cartella 'analysis_results' per i dati dettagliati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a16e5-ff5d-4a18-81f4-01f1eaa845c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
