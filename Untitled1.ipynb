{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eda5399-3ca2-4831-959d-720158b2a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio analisi dei file di performance...\n",
      "\n",
      "DataFrame creato con 467 righe e 14 colonne\n",
      "Colonne: ['operation', 'sparsity', 'run', 'mode', 'vl', 'branch_misses', 'branches', 'context_switches', 'cpu_migrations', 'cycles', 'instructions', 'ipc', 'page_faults', 'task_clock']\n",
      "\n",
      "Prime 5 righe del DataFrame:\n",
      "     operation  sparsity  run    mode  vl  branch_misses  branches  \\\n",
      "0  elementwise      60.0  2.0  scalar NaN        13571.0  131346.0   \n",
      "1  elementwise      60.0  3.0  scalar NaN        13577.0  131097.0   \n",
      "2  elementwise      60.0  1.0  scalar NaN        13511.0  132040.0   \n",
      "3  elementwise      80.0  1.0  scalar NaN        13659.0  128395.0   \n",
      "4  elementwise      80.0  2.0  scalar NaN        13426.0  127347.0   \n",
      "\n",
      "   context_switches  cpu_migrations     cycles  instructions       ipc  \\\n",
      "0               0.0             0.0  3704796.0     1055763.0  0.284972   \n",
      "1               0.0             0.0  3669426.0     1052681.0  0.286879   \n",
      "2               0.0             0.0  3713228.0     1060012.0  0.285469   \n",
      "3               0.0             0.0  3666878.0     1015522.0  0.276945   \n",
      "4               0.0             0.0  3602646.0     1007086.0  0.279541   \n",
      "\n",
      "   page_faults  task_clock  \n",
      "0         67.0        2.07  \n",
      "1         67.0        2.05  \n",
      "2         68.0        2.07  \n",
      "3         66.0        2.05  \n",
      "4         66.0        2.01  \n",
      "\n",
      "Informazioni sul DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 467 entries, 0 to 466\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   operation         467 non-null    object \n",
      " 1   sparsity          62 non-null     float64\n",
      " 2   run               62 non-null     float64\n",
      " 3   mode              467 non-null    object \n",
      " 4   vl                372 non-null    float64\n",
      " 5   branch_misses     467 non-null    float64\n",
      " 6   branches          467 non-null    float64\n",
      " 7   context_switches  467 non-null    float64\n",
      " 8   cpu_migrations    467 non-null    float64\n",
      " 9   cycles            467 non-null    float64\n",
      " 10  instructions      467 non-null    float64\n",
      " 11  ipc               467 non-null    float64\n",
      " 12  page_faults       467 non-null    float64\n",
      " 13  task_clock        467 non-null    float64\n",
      "dtypes: float64(12), object(2)\n",
      "memory usage: 51.2+ KB\n",
      "None\n",
      "\n",
      "Statistiche descrittive per le colonne numeriche:\n",
      "        sparsity        run          vl  branch_misses       branches  \\\n",
      "count  62.000000  62.000000  372.000000     467.000000     467.000000   \n",
      "mean   73.225806   2.064516   15.000000   14227.815846  153532.119914   \n",
      "std    14.796344   0.884678   10.738248    1302.373265   77673.551059   \n",
      "min    50.000000   1.000000    4.000000   13205.000000  113096.000000   \n",
      "25%    60.000000   1.000000    7.000000   13452.000000  126267.500000   \n",
      "50%    75.000000   2.000000   12.000000   13623.000000  128417.000000   \n",
      "75%    85.000000   3.000000   20.000000   14286.500000  141559.500000   \n",
      "max    95.000000   4.000000   32.000000   18774.000000  630772.000000   \n",
      "\n",
      "       context_switches  cpu_migrations        cycles  instructions  \\\n",
      "count             467.0           467.0  4.670000e+02  4.670000e+02   \n",
      "mean                0.0             0.0  5.475392e+06  1.445784e+06   \n",
      "std                 0.0             0.0  2.179279e+06  8.756063e+05   \n",
      "min                 0.0             0.0  3.569962e+06  9.518330e+05   \n",
      "25%                 0.0             0.0  3.775868e+06  1.003677e+06   \n",
      "50%                 0.0             0.0  4.718594e+06  1.126984e+06   \n",
      "75%                 0.0             0.0  6.414534e+06  1.504488e+06   \n",
      "max                 0.0             0.0  1.327199e+07  6.558616e+06   \n",
      "\n",
      "              ipc  page_faults  task_clock  \n",
      "count  467.000000   467.000000  467.000000  \n",
      "mean     0.267195    76.802998    3.053405  \n",
      "std      0.091600     7.395608    1.211343  \n",
      "min      0.162663    65.000000    1.990000  \n",
      "25%      0.220238    67.000000    2.110000  \n",
      "50%      0.255053    82.000000    2.630000  \n",
      "75%      0.269890    82.000000    3.575000  \n",
      "max      0.642989    84.000000    7.390000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Estrae operation, sparsity e run dal nome del file\n",
    "    Es: 'matmul_50_1.csv' -> operation='matmul', sparsity=50, run=1\n",
    "    \"\"\"\n",
    "    # Rimuovi l'estensione .csv\n",
    "    name = filename.replace('.csv', '')\n",
    "    \n",
    "    # Split per underscore\n",
    "    parts = name.split('_')\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        operation = parts[0]\n",
    "        try:\n",
    "            sparsity = int(parts[1])\n",
    "            run = int(parts[2])\n",
    "        except ValueError:\n",
    "            sparsity = np.nan\n",
    "            run = np.nan\n",
    "    else:\n",
    "        operation = parts[0] if parts else ''\n",
    "        sparsity = np.nan\n",
    "        run = np.nan\n",
    "    \n",
    "    return operation, sparsity, run\n",
    "\n",
    "def extract_vl_from_path(path):\n",
    "    \"\"\"\n",
    "    Estrae il valore VL dal path della directory\n",
    "    Es: 'mlir_files_results_vector_16/' -> 16\n",
    "    \"\"\"\n",
    "    # Cerca pattern che termina con _numero\n",
    "    match = re.search(r'_(\\d+)/?$', str(path))\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return np.nan\n",
    "\n",
    "def parse_perf_csv(filepath):\n",
    "    \"\"\"\n",
    "    Analizza un file CSV di perf e restituisce un dizionario con le metriche\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split(',')\n",
    "                if len(parts) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # Il formato è: valore,unità,nome_metrica,...\n",
    "                try:\n",
    "                    value = float(parts[0])\n",
    "                    metric_name = parts[2]\n",
    "                    \n",
    "                    # Pulisci il nome della metrica\n",
    "                    metric_name = metric_name.replace('-', '_')\n",
    "                    \n",
    "                    # Gestisci casi speciali per nomi di metriche\n",
    "                    if metric_name == 'task_clock':\n",
    "                        metrics['task_clock'] = value\n",
    "                    elif metric_name == 'context_switches':\n",
    "                        metrics['context_switches'] = value\n",
    "                    elif metric_name == 'cpu_migrations':\n",
    "                        metrics['cpu_migrations'] = value\n",
    "                    elif metric_name == 'page_faults':\n",
    "                        metrics['page_faults'] = value\n",
    "                    elif metric_name == 'cycles':\n",
    "                        metrics['cycles'] = value\n",
    "                    elif metric_name == 'instructions':\n",
    "                        metrics['instructions'] = value\n",
    "                    elif metric_name == 'branches':\n",
    "                        metrics['branches'] = value\n",
    "                    elif metric_name == 'branch_misses':\n",
    "                        metrics['branch_misses'] = value\n",
    "                    else:\n",
    "                        # Per altre metriche, usa il nome così com'è\n",
    "                        metrics[metric_name] = value\n",
    "                        \n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel leggere {filepath}: {e}\")\n",
    "    \n",
    "    # Calcola IPC se abbiamo sia instructions che cycles\n",
    "    if 'instructions' in metrics and 'cycles' in metrics and metrics['cycles'] > 0:\n",
    "        metrics['ipc'] = metrics['instructions'] / metrics['cycles']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def analyze_performance_data():\n",
    "    \"\"\"\n",
    "    Funzione principale per analizzare i dati di performance\n",
    "    \"\"\"\n",
    "    # Directory principali da analizzare\n",
    "    base_dirs = ['results_scalar/', 'results_vector/']\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for base_dir in base_dirs:\n",
    "        if not os.path.exists(base_dir):\n",
    "            print(f\"Directory {base_dir} non trovata, salto...\")\n",
    "            continue\n",
    "        \n",
    "        # Determina il mode dalla directory\n",
    "        mode = 'scalar' if 'scalar' in base_dir else 'vector'\n",
    "        \n",
    "        # Attraversa ricorsivamente la directory\n",
    "        for root, dirs, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    \n",
    "                    # Estrai informazioni dal nome del file\n",
    "                    operation, sparsity, run = parse_filename(file)\n",
    "                    \n",
    "                    # Estrai VL se siamo in mode vector\n",
    "                    vl = np.nan\n",
    "                    if mode == 'vector':\n",
    "                        vl = extract_vl_from_path(root)\n",
    "                    \n",
    "                    # Analizza il file CSV\n",
    "                    metrics = parse_perf_csv(filepath)\n",
    "                    \n",
    "                    # Crea il record\n",
    "                    record = {\n",
    "                        'operation': operation,\n",
    "                        'sparsity': sparsity,\n",
    "                        'run': run,\n",
    "                        'mode': mode,\n",
    "                        'vl': vl,\n",
    "                        **metrics  # Aggiungi tutte le metriche\n",
    "                    }\n",
    "                    \n",
    "                    all_data.append(record)\n",
    "    \n",
    "    # Crea il DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Riordina le colonne mettendo i metadati per primi\n",
    "    meta_cols = ['operation', 'sparsity', 'run', 'mode', 'vl']\n",
    "    metric_cols = [col for col in df.columns if col not in meta_cols]\n",
    "    \n",
    "    # Ordina le colonne metriche alfabeticamente\n",
    "    metric_cols.sort()\n",
    "    \n",
    "    df = df[meta_cols + metric_cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Esegui l'analisi\n",
    "print(\"Inizio analisi dei file di performance...\")\n",
    "df = analyze_performance_data()\n",
    "\n",
    "print(f\"\\nDataFrame creato con {len(df)} righe e {len(df.columns)} colonne\")\n",
    "print(f\"Colonne: {list(df.columns)}\")\n",
    "\n",
    "print(\"\\nPrime 5 righe del DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nInformazioni sul DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nStatistiche descrittive per le colonne numeriche:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44968e11-139e-46ca-ae52-de16fe33e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio elaborazione del DataFrame...\n",
      "=== PULIZIA E CONVERSIONE DATI ===\n",
      "DataFrame originale: (467, 14)\n",
      "Colonne originali: ['operation', 'sparsity', 'run', 'mode', 'vl', 'branch_misses', 'branches', 'context_switches', 'cpu_migrations', 'cycles', 'instructions', 'ipc', 'page_faults', 'task_clock']\n",
      "Rimozione colonne: ['operation', 'cpu_migrations']\n",
      "Conversione di 8 colonne numeriche...\n",
      "Calcolo branch_miss_rate = branch_misses / branches...\n",
      "Calcolo frequency_ghz = cycles / (task_clock * 1e9)...\n",
      "DataFrame pulito: (467, 14)\n",
      "Colonne disponibili dopo pulizia: ['sparsity', 'run', 'mode', 'vl', 'branch_misses', 'branches', 'context_switches', 'cycles', 'instructions', 'ipc', 'page_faults', 'task_clock', 'branch_miss_rate', 'frequency_ghz']\n",
      "\n",
      "Tipi di dati:\n",
      "sparsity            float64\n",
      "run                 float64\n",
      "mode                 object\n",
      "vl                  float64\n",
      "branch_misses       float64\n",
      "branches            float64\n",
      "context_switches    float64\n",
      "cycles              float64\n",
      "instructions        float64\n",
      "ipc                 float64\n",
      "page_faults         float64\n",
      "task_clock          float64\n",
      "branch_miss_rate    float64\n",
      "frequency_ghz       float64\n",
      "dtype: object\n",
      "\n",
      "=== DISTRIBUZIONI ===\n",
      "\n",
      "Colonna 'operation' non presente nel DataFrame\n",
      "\n",
      "Distribuzione per mode:\n",
      "mode\n",
      "vector    374\n",
      "scalar     93\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuzione per sparsity:\n",
      "sparsity\n",
      "95.0    8\n",
      "60.0    6\n",
      "80.0    6\n",
      "85.0    6\n",
      "65.0    6\n",
      "70.0    6\n",
      "90.0    6\n",
      "50.0    6\n",
      "55.0    6\n",
      "75.0    6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuzione per vl:\n",
      "vl\n",
      "16.0    93\n",
      "8.0     93\n",
      "4.0     93\n",
      "32.0    93\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== AGGREGAZIONE DATI ===\n",
      "Raggruppamento per: ['sparsity', 'vl', 'mode', 'run']\n",
      "Calcolo media per 10 metriche: ['branch_misses', 'branches', 'context_switches', 'cycles', 'instructions', 'ipc', 'page_faults', 'task_clock', 'branch_miss_rate', 'frequency_ghz']\n",
      "DataFrame aggregato: (0, 16)\n",
      "Combinazioni uniche: 0\n",
      "\n",
      "=== RISULTATO FINALE ===\n",
      "DataFrame finale: (0, 16)\n",
      "Colonne: ['sparsity', 'vl', 'mode', 'run', 'branch_misses', 'branches', 'context_switches', 'cycles', 'instructions', 'ipc', 'page_faults', 'task_clock', 'branch_miss_rate', 'frequency_ghz', 'total_runs', 'unique_runs']\n",
      "\n",
      "DataFrame aggregato vuoto\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_and_process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Pulisce e processa il DataFrame di performance\n",
    "    \"\"\"\n",
    "    # Crea una copia per non modificare l'originale\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    print(\"=== PULIZIA E CONVERSIONE DATI ===\")\n",
    "    print(f\"DataFrame originale: {df_clean.shape}\")\n",
    "    print(f\"Colonne originali: {list(df_clean.columns)}\")\n",
    "    \n",
    "    # 1. Identifica e rimuovi colonne inutili/ridondanti\n",
    "    columns_to_remove = []\n",
    "    \n",
    "    # Pattern per identificare colonne da rimuovere\n",
    "    remove_patterns = [\n",
    "        r'.*\\/sec$',  # Colonne che finiscono con /sec\n",
    "        r'.*CPUs.*utilized.*',  # CPUs utilized\n",
    "        r'.*percentage.*',  # Percentuali\n",
    "        r'.*ratio.*',  # Rapporti ridondanti\n",
    "        r'.*time.*unit.*',  # Unità di tempo ridondanti\n",
    "    ]\n",
    "    \n",
    "    for col in df_clean.columns:\n",
    "        if any(re.search(pattern, str(col), re.IGNORECASE) for pattern in remove_patterns):\n",
    "            columns_to_remove.append(col)\n",
    "    \n",
    "    # Rimuovi colonne identificate\n",
    "    if columns_to_remove:\n",
    "        print(f\"Rimozione colonne: {columns_to_remove}\")\n",
    "        df_clean = df_clean.drop(columns=columns_to_remove)\n",
    "    \n",
    "    # 2. Converti colonne numeriche da stringa a float\n",
    "    # Identifica colonne che dovrebbero essere numeriche (esclusi metadati)\n",
    "    # Prima verifica quali colonne di metadati esistono effettivamente\n",
    "    possible_metadata_cols = ['operation', 'sparsity', 'run', 'mode', 'vl']\n",
    "    metadata_cols = [col for col in possible_metadata_cols if col in df_clean.columns]\n",
    "    \n",
    "    numeric_cols = [col for col in df_clean.columns if col not in metadata_cols]\n",
    "    \n",
    "    print(f\"Conversione di {len(numeric_cols)} colonne numeriche...\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df_clean.columns:\n",
    "            # Converti a stringa prima per gestire eventuali NaN\n",
    "            df_clean[col] = df_clean[col].astype(str)\n",
    "            \n",
    "            # Rimuovi caratteri non numerici (tranne punto e segno meno)\n",
    "            df_clean[col] = df_clean[col].str.replace(r'[^\\d.-]', '', regex=True)\n",
    "            \n",
    "            # Sostituisci stringhe vuote con NaN\n",
    "            df_clean[col] = df_clean[col].replace('', np.nan)\n",
    "            \n",
    "            # Converti a float\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    \n",
    "    # 3. Calcola IPC se non presente e se abbiamo instructions e cycles\n",
    "    if 'ipc' not in df_clean.columns:\n",
    "        if 'instructions' in df_clean.columns and 'cycles' in df_clean.columns:\n",
    "            print(\"Calcolo IPC = instructions / cycles...\")\n",
    "            df_clean['ipc'] = df_clean['instructions'] / df_clean['cycles']\n",
    "            # Sostituisci infiniti con NaN\n",
    "            df_clean['ipc'] = df_clean['ipc'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # 4. Calcola altre metriche derivate utili\n",
    "    # Calcola miss rate per branch se abbiamo entrambe le metriche\n",
    "    if 'branch_misses' in df_clean.columns and 'branches' in df_clean.columns:\n",
    "        print(\"Calcolo branch_miss_rate = branch_misses / branches...\")\n",
    "        df_clean['branch_miss_rate'] = df_clean['branch_misses'] / df_clean['branches']\n",
    "        df_clean['branch_miss_rate'] = df_clean['branch_miss_rate'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Calcola frequenza media se abbiamo cycles e task_clock\n",
    "    if 'cycles' in df_clean.columns and 'task_clock' in df_clean.columns:\n",
    "        print(\"Calcolo frequency_ghz = cycles / (task_clock * 1e9)...\")\n",
    "        # task_clock è in secondi, vogliamo GHz\n",
    "        df_clean['frequency_ghz'] = df_clean['cycles'] / (df_clean['task_clock'] * 1e9)\n",
    "        df_clean['frequency_ghz'] = df_clean['frequency_ghz'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    print(f\"DataFrame pulito: {df_clean.shape}\")\n",
    "    print(f\"Colonne disponibili dopo pulizia: {list(df_clean.columns)}\")\n",
    "    return df_clean\n",
    "\n",
    "def aggregate_dataframe(df_clean):\n",
    "    \"\"\"\n",
    "    Aggrega il DataFrame facendo la media sui run\n",
    "    \"\"\"\n",
    "    print(\"\\n=== AGGREGAZIONE DATI ===\")\n",
    "    \n",
    "    # Colonne di raggruppamento - usa solo quelle che esistono nel DataFrame\n",
    "    possible_groupby_cols = ['operation', 'sparsity', 'vl', 'mode', 'run']\n",
    "    groupby_cols = [col for col in possible_groupby_cols if col in df_clean.columns]\n",
    "    \n",
    "    if not groupby_cols:\n",
    "        print(\"Nessuna colonna di raggruppamento trovata - restituisco DataFrame non aggregato\")\n",
    "        return df_clean\n",
    "    \n",
    "    # Identifica colonne numeriche per l'aggregazione\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Rimuovi le colonne di raggruppamento dalle numeriche\n",
    "    metric_cols = [col for col in numeric_cols if col not in groupby_cols]\n",
    "    \n",
    "    print(f\"Raggruppamento per: {groupby_cols}\")\n",
    "    print(f\"Calcolo media per {len(metric_cols)} metriche: {metric_cols}\")\n",
    "    \n",
    "    # Esegui il groupby e calcola la media\n",
    "    df_grouped = df_clean.groupby(groupby_cols)[metric_cols].mean().reset_index()\n",
    "    \n",
    "    # Aggiungi informazioni sui run aggregati se esiste la colonna 'run'\n",
    "    if 'run' in df_clean.columns:\n",
    "        run_counts = df_clean.groupby(groupby_cols)['run'].agg(['count', 'nunique']).reset_index()\n",
    "        run_counts.columns = groupby_cols + ['total_runs', 'unique_runs']\n",
    "        \n",
    "        # Merge con il DataFrame aggregato\n",
    "        df_grouped = df_grouped.merge(run_counts, on=groupby_cols, how='left')\n",
    "    \n",
    "    print(f\"DataFrame aggregato: {df_grouped.shape}\")\n",
    "    print(f\"Combinazioni uniche: {len(df_grouped)}\")\n",
    "    \n",
    "    return df_grouped\n",
    "\n",
    "# Esegui la pulizia e il processing\n",
    "print(\"Inizio elaborazione del DataFrame...\")\n",
    "df_clean = clean_and_process_dataframe(df)\n",
    "\n",
    "print(f\"\\nTipi di dati:\")\n",
    "print(df_clean.dtypes)\n",
    "\n",
    "# Mostra alcune statistiche pre-aggregazione solo per le colonne esistenti\n",
    "print(\"\\n=== DISTRIBUZIONI ===\")\n",
    "for col in ['operation', 'mode', 'sparsity', 'vl']:\n",
    "    if col in df_clean.columns:\n",
    "        print(f\"\\nDistribuzione per {col}:\")\n",
    "        print(df_clean[col].value_counts())\n",
    "    else:\n",
    "        print(f\"\\nColonna '{col}' non presente nel DataFrame\")\n",
    "\n",
    "# Esegui l'aggregazione\n",
    "df_grouped = aggregate_dataframe(df_clean)\n",
    "\n",
    "print(f\"\\n=== RISULTATO FINALE ===\")\n",
    "print(f\"DataFrame finale: {df_grouped.shape}\")\n",
    "print(f\"Colonne: {list(df_grouped.columns)}\")\n",
    "\n",
    "if len(df_grouped) > 0:\n",
    "    print(f\"\\nPrime 5 righe del DataFrame aggregato:\")\n",
    "    print(df_grouped.head())\n",
    "\n",
    "    print(f\"\\nStatistiche descrittive delle metriche aggregate:\")\n",
    "    numeric_cols = df_grouped.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(df_grouped[numeric_cols].describe())\n",
    "    else:\n",
    "        print(\"Nessuna colonna numerica disponibile per le statistiche\")\n",
    "\n",
    "    # Salva anche una versione ordinata\n",
    "    sort_cols = [col for col in ['operation', 'mode', 'sparsity', 'vl'] if col in df_grouped.columns]\n",
    "    if sort_cols:\n",
    "        df_grouped = df_grouped.sort_values(sort_cols).reset_index(drop=True)\n",
    "        print(f\"\\nDataFrame ordinato per {sort_cols}\")\n",
    "        \n",
    "        if 'operation' in df_grouped.columns and 'mode' in df_grouped.columns:\n",
    "            print(f\"\\nRaggruppamento per operation-mode:\")\n",
    "            print(df_grouped.groupby(['operation', 'mode']).size())\n",
    "else:\n",
    "    print(\"\\nDataFrame aggregato vuoto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ab8b5-87bd-4b15-b77f-0fcfad84f3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
